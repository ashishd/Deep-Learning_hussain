{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    },
    "colab": {
      "name": "Introduction_of_Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussain0048/Deep-Learning-with-Keras/blob/master/Introduction_of_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WZFQQORCAcS",
        "colab_type": "text"
      },
      "source": [
        "# **1-Introduction Keras**[1]\n",
        "## 1.1 - **What is keras**\n",
        "\n",
        "Keras is a deep-learning framework that provides a Convenient way to define and train almost any kind of Deep leanring model. It is written in Python and can be run on top of Tensorflow , CNTK, or Theano. you are free to use it in commerical projects since it is distributed under the MIT licens [1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-OS-JtDaAzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/hussain0048/Deep-Learning-with-Keras.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5li5TZBz4NQ",
        "colab_type": "text"
      },
      "source": [
        "## 1.2- **What makes Keras so popular** \n",
        "once of the most important characteristic of kearas is it user-friendly API. You could develop a start of art DL Model in no time . Therefore it is easy and fast prototyping.In addition, it support many modern DL layers such as Convolutional and recurrent layers.Keras layers can be added sequentially or many different combinations in very easy wasy. Regarding hardware, you can run keras on CPU and GPU and switch between them in very easy way[1] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beVu-1UqCAcU",
        "colab_type": "text"
      },
      "source": [
        "## 1.3-  **Installing Keras** \n",
        "The installation process is very easy. First, we need to install the backedn where  all the calculation take place (we will choose tensorFlow). Then we install keras [1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF03EKpuCAcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLq1Pb9z9bk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ktand8m9iKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# it is simple as this. Let us test the implementation \n",
        "python -c 'import keras; print(keras._version_)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnY7VuCDI-qN",
        "colab_type": "text"
      },
      "source": [
        "## 1.4- **Kera Workflow**#\n",
        "\n",
        "In order to build DL project in keras you normally would follow the following workk flow [1]:\n",
        "- 1) Define your training data\n",
        "- 2) Define your network \n",
        "- 3) Configure the learning process by choosing \n",
        "      - 1) optimizer \n",
        "      - 2) Metrics \n",
        "- 4) iterate over the training data and start fitting your model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a21M_iV6uN_M",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1eVwjExOmSWHL6nFcvsWhCxy6aokQNXQq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_aCyhDgoTRK",
        "colab_type": "text"
      },
      "source": [
        "## 1.5- **Keras Models?** \n",
        "The core data structure of keras is the model class. it is found under keras.model that gives you two ways to define models:\n",
        " - Sequential class\n",
        " - Model class\n",
        "\n",
        "The sequential class builds the network layers by layers  in a sequential order . The model class allows for more Complext network structures[1]. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq4DpmTeRNNy",
        "colab_type": "text"
      },
      "source": [
        "## 1.6- **Model lifecycle**\n",
        "A Keras model follows the following lifecycle [1]\n",
        "\n",
        " -1. Modle creation\n",
        "\n",
        "     - Define a model using the sequential or model class\n",
        "     - Add the layers \n",
        "\n",
        " -2. Configure the model by specifying the loss, optimizer and metric. This is done by calling the compile method\n",
        "\n",
        " -3. Train the model by calling the fit method\n",
        " \n",
        " -4. By then you will have a trained model that you could use for evaluation or prediction on new data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOsOIr2HuwEu",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1QghXXdHiADg72Szq7843cDaXKkp7fRUW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcVaahYFRsEF",
        "colab_type": "text"
      },
      "source": [
        "## 1.7- **Core layers**\n",
        "keras supports many layers for building our neural network. They accessible from **keras.layers** and the following shows the most basic classes we are going to use [1]:\n",
        "\n",
        " - Dense: is the standard layer of fully connected neuron to the pervious layer. It implemented the operation output =activation (X*W+Bias)\n",
        " - Activation: Applies an activation function to an output \n",
        " - Dropout: applies dropout to the input. Basically , it work randomly deactivation a set of neurons in a given layer according to a predefined probability rate. Drouput is used to prevent overfitting \n",
        " - Conv2D: applies a 2D Convolution to train a set of kernels mainly on image database\n",
        " - Flatten: Flattens the input into ID matrix. Mainly used after feature extraction in Convolution Neural network     \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C_dTcsnSGFD",
        "colab_type": "text"
      },
      "source": [
        "## 1.8-**Loss and Optimizers**\n",
        "After defining a model, we need to select a loss function and an optimizer. The optimizer's jobs is to find the best model parameters that minimizes the losss function[1].\n",
        "\n",
        "Avalilable optimizers: SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
        "\n",
        "Available loss Functions: mean absolut error, mean absolute percentage error, mean squared. Logarithmic error , Squared hing, Categorical hinge, logcosh,categorical crossentropy, sparse Categorical crossentropy, binary corssentropy , kullback divergence, poisson, costine proximity \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWjlZqTBTeD7",
        "colab_type": "text"
      },
      "source": [
        "## 1.9. **Keras Utils** \n",
        "Keras provides additional utility functions that facilitates building and viewing models. We will mainly use the them to preprocess data and viewing models [1] \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOLixKnwiYkE",
        "colab_type": "text"
      },
      "source": [
        "#**2-Data Pre-processing**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNBeXQIhvbKB",
        "colab_type": "text"
      },
      "source": [
        "## **2.1-Dealing with Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwuJEzGMvqDe",
        "colab_type": "text"
      },
      "source": [
        "**Motivation**\n",
        "\n",
        "Training deep learning models requires dataâ€¦ A lot of data! Unfortunately, in most cases data comes messy, and our models are very sensitive towards this. Therefore, we need to be careful while preparing our data to achieve the best results.Get your laptops ready, we have a lot of preprocessing to do [3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKcl915YwRoh",
        "colab_type": "text"
      },
      "source": [
        "## **2.3 Working with Numerical Data**\n",
        "\n",
        "Numerical values are going to be the most frequent data types you are going to deal with. Even though they are already in a suitable format for calculations, we still need to do some work.\n",
        "\n",
        "The main problem with numerical data is the different scales each feature holds. Consider a housing prices dataset with information about: house size, number of bedrooms, construction year, and price. Suppose our goal is to predict the price given the house size, number of bedrooms and construction year. Each of those features is presented on a different scale. A house size may range let us say between 100 and 500 meters squared, construction year is a 4 digits number that goes back around 200 years ago, and finally the number of bedrooms is a number between 1-4 [3].\n",
        "\n",
        "The issue here is that a model may give more attention to one feature based on its value. This way the house size may get more attention since its values are bigger, and other important features such as number of bedrooms may be neglected since their values are small [3].\n",
        "\n",
        "Well, do not panic! We have two simple solutions for this problem, they are called Normalization and Standardization [3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGMKB3xlxPkb",
        "colab_type": "text"
      },
      "source": [
        "**Normalization**\n",
        "\n",
        "Normalization simply scales the values in the range [0-1]. To apply it on a dataset you just have to subtract the minimum value from each feature and divide it with the range (max â€“ min).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8TZNiORxvHy",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1DsPxs-4x_BTRfXzjE8Pu_O9dsCYfshff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63E6i9mcx45k",
        "colab_type": "text"
      },
      "source": [
        "**Standardization**\n",
        "\n",
        "Standardization on the other hand transforms data to have a zero mean and one unit standard deviation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAhobx7lyJq1",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1jaqkqDWPNebx1rrp191w8D4NB5IkbwqK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cha3yOU2y06q",
        "colab_type": "text"
      },
      "source": [
        "**Implementation**\n",
        "\n",
        "Implementing the above techniques in Keras is easier than you think. We will show you an example using the Boston Housing dataset that can be easily loaded with Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROO7qWcpy9a0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        "# data is returned as a tuple for the training and the testing datasets\n",
        "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNvfq35MzLkU",
        "colab_type": "text"
      },
      "source": [
        "Let us look at the first example in the training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-dMYQKpzOqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnphcnsdzYWc",
        "colab_type": "text"
      },
      "source": [
        "See the different scales? To solve this we will use the popular Scikit-Learn library.\n",
        "\n",
        "Use the MinMaxScaler for data normalization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v2Gv7bizfPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(X_train)\n",
        "print(X_normalized[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKDG6Zh-zvYj",
        "colab_type": "text"
      },
      "source": [
        "OR, use the StandardScaler to standardize:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8RC5Wk1zxE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_train)\n",
        "print(X_scaled[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WngdvLztz8Qt",
        "colab_type": "text"
      },
      "source": [
        "## **2.4 Working with Categorical Data**[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm7uTbEWCAei",
        "colab_type": "text"
      },
      "source": [
        "#**References**\n",
        "\n",
        " [1] Deep learning with keras Tutorila part 1\n",
        "https://www.marktechpost.com/2019/06/11/deep-learning-with-keras-tutorial-part-1/?fbclid=IwAR0Mbtuas8dNItrxRQvAmZG2OwYXOe6JkjTLmZUNFokVcDyLYHIlENSOze0\n",
        "\n",
        "[2] Regression with Keras (Deep Learning with Keras â€“ Part 3)\n",
        "https://www.marktechpost.com/2019/06/17/regression-with-keras-deep-learning-with-keras-part-3/?fbclid=IwAR0WEGYwi1nlfK_eaH1qK3JD8ITMYc82Q15uT_vC4DnNUOgJboEn8KLr5_Y\n",
        "\n",
        "[3] Data Pre-processing for Deep Learning models (Deep Learning with Keras â€“ Part 2)\n",
        "https://www.marktechpost.com/2019/06/14/data-pre-processing-for-deep-learning-models-deep-learning-with-keras-part-2/?fbclid=IwAR22EF2_QJDOXwc1bFrgiA-ciKzwkbznmcXm8rf8xMJSqkZpKaAsay-_ZuU\n"
      ]
    }
  ]
}